You can find virtually all of the defects that a comprehensive test approach would find while using orders of magnitude less testing time and resources. This technique is called combinatorial testing. Testing all possible pairs of values is a kind of combinatorial testing, but has its own term, pairwise testing or all-pairs testing.

## Stochastic and Property-Based Testing

Stochastic testing is black box testing, random testing, performed by automated testing tools. Stochastic testing is a series of random tests over time. Stochastic testing is also referred to as monkey testing.

Since you - or more precisely, the stochastic testing system- may not know exactly what the expected behavior should be for a given input, you need to check for properties of the system. At the unit testing level, where you are checking individual methods, this is called **property-based testing**

### Smart, Dumb, Evil and Chaos Monkeys

- **Dumb Monkey** testing is sending in just any old input you can think of.
- **Smart monkey** - testing involves using input which a user might conceivably enter, as opposed to being strictly random. Creating a smart monkey test can be difficult, because not only do you have to first understand what users would likely do with the application, but then develop a model for it. However, the smart monkey is much more likely to discover a defect in the system under test.
- **Evil monkey** - testing simulates a malicious user who is actively trying to hurt your system. This can be through sending very long strings, potential injection attacks, malformed data, or other inputs which are designed to cause havoc in your system. In today's networked world, systems are almost always under attack if they are connected to the Internet.
- **Chaos Monkey** - is a tool developed by Netflix which randomly shuts down servers that their system is running on, in order to simulate random outages. For any large system, servers will go down on a regular basis, and at any given time, some percentage of systems will be unavailable. Chaos monkey testing ensures that the system as a whole will be able to operate effectively when when individual machines are not responding.

## Performance Testing

How you measure performance will be based upon the kind of system you are testing. That does not mean, however, that there are no rules or heuristics.

### Categories of Performance Indicators

Defining performance for a particular system means determining which performance indicators the user or customer cares about. Although there are a large number of possible perfoemance indicators, there are two main categories: service-oriented and efficiency-oriented.

A **service-oriented indicator** measures how well a system is providing a service to a particular user. These are measured from the user's perspective and quantify aspects of the system the user would directly care about. Some examples of service-oriented indicators would be average page load time, response time, or what percentage of the time the system is available.There are two main subcategories of service-oriented indicators. The first is availability—how available is the system to the user? This could mean anything from what percentage of the time can the system be accessed to how available different features are to the users at different times. The second main category is response time—how long does it take for the system to respond to the user’s input or return a result?

An **efficiency-oriented indicator** measures how efficiently a system makes use of available computational resources. You can think of these as being measured "from the program's point of view" or at least from the computer's point of view. Examples of efficiency-oriented indicators would be what percentage of CPU operations are being used for performing some functionality, how much disk space is being used, or how many concurrent users the system can handle on a particular server. Just like service-oriented indicators, there are two main subcategories in efficiency-oriented testing. Throughput measures how many events can the system handle in a given amount of time, such as how many users can log in at the same time, or how many packets can a system route in 30 seconds. Utilization measures what percentage or absolute amount of computing resources are used to perform a given task. For example, how much disk space does a database take to store a specified table? How much RAM is necessary when sorting it?

### Testing Performance: Thresholds and Targets

In order to determine whether or not a performance test has "passed", you need **performance targets** or specific quantitative values that the performance indicatos are supposed to reach

Targets, though, are the ideal. Oftentimes, a particular performance indicator for a system may not reach its target, or may not be able to. Performance thresholds indicate the point where a performance indicator reaches an absolutely minimal acceptable performance. For example, while the target for response time may be 500 milliseconds, systems engineers may have determined that the system would be releasable with a response time of 3000 milliseconds. It would not be great if it only met that mark, but if it takes any longer, the system would probably not be releasable. A system whose performance indicators merely meet the threshold should not be considered as “passing” that particular performance metric. There is still work to do! You can see that the standard “pass”/“fail” metric often used for functional testing is often not really appropriate for performance metrics. There are shades of gray, and they are often able to be manipulated. What kind of hardware are you running the tests on? What operating system? What other processes are running? Which web browser are you using to access the site? By changing these, the same system may show very different results on a performance test.

**Load testing** - In load testing, a system is set up and monitored while events are processed. The kind of events will depend on the system under test - for a webserver, for example, it might be page loads, or for a router, it would be packets routed. By using different kinds of load tests based on realistic assumptions, one can determine the MTBF and MTTR and thus the availability that users can expect to see.

A **baseline test** runs the system with few or no events to process, just to check that the system can in fact handle running for a given period of time. This is often not a realistic scenario—a system which never does much is not likely to be developed in the first place - but provides a "baseline" which future tests can compare results against. The opposite of a baseline test is a stress test, in which the system is "stressed" by having it process a large number of events in a small time frame.

A **stability test**, sometimes also called a soak test, is often the single most realistic load test that is run. During this kind of load test, a constant but small-to-medium number of events are processed over a long period of time, in order to determine that the system is in fact stable when doing realistic work.

### Testing Efficiency-Oriented Indicators: Throughput

One measure of efficiency is the amount of throughput, or number of events that can be processed in a given amount of time on a specified hardware setup. We can use load testing to determine the throughput of the system. Instead of determining if the system is up however, we can specifically check for what sustained rate of events can occur before some predetermined lower threshold on performance is reached.

### Testing Efficiency-Oriented Indicators: Utilization

Testing utilization means determining what amount of computing resources - on an absolute or relative basis- a particular system uses when performing some functionality. CPU, Disk usage, RAM, Network usage.

A **profiler** will allow you to not only see that the process is using lots of CPU, but how much CPU each particular method is using.

### General Tips and Advice When Performance Testing

1. Use a tool for any non-trivial performance measurement. Relying on humans to execute many performance tests will lead to mistakes due to human error,as well as increasing the possibility of demoralized testers.
2. Compare apples to apples; keep variables the same between runs of the same test! Don’t run the first version of the software on a laptop and the second version on a supercomputer and proclaim a massive increase in speed.
3. Be mindful of startup and/or shutdown costs. If a system needs to start up a VM, then you may need a way to standardize that across all runs, ignore the first result since it will include startup times, or otherwise take it into consideration.
4. Be mindful of caching. If a system is caching results, you may see very different performance measurements the first time running a test versus the second and third times.
5. Have control over the testing system
6. Have good input data. If you can, try to test with real, or at least possibly real data. Many times the kind of performance that you see on a system will depend upon data.
7. Don't trust a single test run. Although you endeavor to remove all extraneous variables from your performance test, there will always be elements you can’t control, from how the memory is allocated to which order threads are run. While these may or may not have a significant impact on the performance of your system, they are always there. The only way to minimize their impact is to run the test multiple times and hopefully smooth out the data by taking the mean result or otherwise analyzing it statistically.
8. Keep track of your test runs, and store the data. This will allow you to determine when problems first started appearing, problems which you may not have noticed at first glance. It will also let you determine trends. For example, you may notice that with each successive version of the software you test, memory usage is going up while CPU usage is going down.
9. Finally, consider how much performance testing is necessary and that you have to do.


